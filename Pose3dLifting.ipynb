{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pose3dLifting.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4h6oVQM307Ui/vNLXnLZ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArimaValanImmanuel/P3DL/blob/main/Pose3dLifting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSH7a0X1p3-r"
      },
      "source": [
        "# Tensorflow 1.13.x\r\n",
        "\r\n",
        "and other necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGYi1MDVUVZr"
      },
      "source": [
        "!pip uninstall -y tensorflow\r\n",
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmQXwm8HUawy"
      },
      "source": [
        "!pip freeze | grep -i -e tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIwAUCK2spKc"
      },
      "source": [
        "!pip uninstall imgaug\r\n",
        "!pip install imgaug==0.2.6\r\n",
        "!pip install -q -U opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_s9INGJqGC4"
      },
      "source": [
        "# Package/Lifting/Utils\r\n",
        "\r\n",
        "1.config\r\n",
        "2.cpm\r\n",
        "3.draw\r\n",
        "4.prob_model\r\n",
        "5.process\r\n",
        "6.upright_fast\r\n",
        "\r\n",
        "#Package/Lifting\r\n",
        "\r\n",
        "_pose_estimator\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCuwuxPq47e"
      },
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEmWcyqVqwzN"
      },
      "source": [
        "__all__ = [\r\n",
        "    'VISIBLE_PART',\r\n",
        "    'MIN_NUM_JOINTS',\r\n",
        "    'CENTER_TR',\r\n",
        "    'SIGMA',\r\n",
        "    'STRIDE',\r\n",
        "    'SIGMA_CENTER',\r\n",
        "    'INPUT_SIZE',\r\n",
        "    'OUTPUT_SIZE',\r\n",
        "    'NUM_JOINTS',\r\n",
        "    'NUM_OUTPUT',\r\n",
        "    'H36M_NUM_JOINTS',\r\n",
        "    'JOINT_DRAW_SIZE',\r\n",
        "    'LIMB_DRAW_SIZE'\r\n",
        "]\r\n",
        "\r\n",
        "# threshold\r\n",
        "VISIBLE_PART = 1e-3\r\n",
        "MIN_NUM_JOINTS = 5\r\n",
        "CENTER_TR = 0.4\r\n",
        "\r\n",
        "# net attributes\r\n",
        "SIGMA = 7\r\n",
        "STRIDE = 8\r\n",
        "SIGMA_CENTER = 21\r\n",
        "INPUT_SIZE = 368\r\n",
        "OUTPUT_SIZE = 46\r\n",
        "NUM_JOINTS = 14\r\n",
        "NUM_OUTPUT = NUM_JOINTS + 1\r\n",
        "H36M_NUM_JOINTS = 17\r\n",
        "\r\n",
        "# draw options\r\n",
        "JOINT_DRAW_SIZE = 3\r\n",
        "LIMB_DRAW_SIZE = 2\r\n",
        "NORMALISATION_COEFFICIENT = 1280*720\r\n",
        "\r\n",
        "# test options\r\n",
        "BATCH_SIZE = 4\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlKmcmS6q9g_"
      },
      "source": [
        "cpm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fEtnBv4q8yi"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow.contrib.layers as layers\r\n",
        "\r\n",
        "__all__ = [\r\n",
        "    'inference_person',\r\n",
        "    'inference_pose'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def inference_person(image):\r\n",
        "    with tf.variable_scope('PersonNet'):\r\n",
        "        conv1_1 = layers.conv2d(\r\n",
        "            image, 64, 3, 1, activation_fn=None, scope='conv1_1')\r\n",
        "        conv1_1 = tf.nn.relu(conv1_1)\r\n",
        "        conv1_2 = layers.conv2d(\r\n",
        "            conv1_1, 64, 3, 1, activation_fn=None, scope='conv1_2')\r\n",
        "        conv1_2 = tf.nn.relu(conv1_2)\r\n",
        "        pool1_stage1 = layers.max_pool2d(conv1_2, 2, 2)\r\n",
        "        conv2_1 = layers.conv2d(pool1_stage1, 128, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv2_1')\r\n",
        "        conv2_1 = tf.nn.relu(conv2_1)\r\n",
        "        conv2_2 = layers.conv2d(\r\n",
        "            conv2_1, 128, 3, 1, activation_fn=None, scope='conv2_2')\r\n",
        "        conv2_2 = tf.nn.relu(conv2_2)\r\n",
        "        pool2_stage1 = layers.max_pool2d(conv2_2, 2, 2)\r\n",
        "        conv3_1 = layers.conv2d(pool2_stage1, 256, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv3_1')\r\n",
        "        conv3_1 = tf.nn.relu(conv3_1)\r\n",
        "        conv3_2 = layers.conv2d(\r\n",
        "            conv3_1, 256, 3, 1, activation_fn=None, scope='conv3_2')\r\n",
        "        conv3_2 = tf.nn.relu(conv3_2)\r\n",
        "        conv3_3 = layers.conv2d(\r\n",
        "            conv3_2, 256, 3, 1, activation_fn=None, scope='conv3_3')\r\n",
        "        conv3_3 = tf.nn.relu(conv3_3)\r\n",
        "        conv3_4 = layers.conv2d(\r\n",
        "            conv3_3, 256, 3, 1, activation_fn=None, scope='conv3_4')\r\n",
        "        conv3_4 = tf.nn.relu(conv3_4)\r\n",
        "        pool3_stage1 = layers.max_pool2d(conv3_4, 2, 2)\r\n",
        "        conv4_1 = layers.conv2d(pool3_stage1, 512, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv4_1')\r\n",
        "        conv4_1 = tf.nn.relu(conv4_1)\r\n",
        "        conv4_2 = layers.conv2d(\r\n",
        "            conv4_1, 512, 3, 1, activation_fn=None, scope='conv4_2')\r\n",
        "        conv4_2 = tf.nn.relu(conv4_2)\r\n",
        "        conv4_3 = layers.conv2d(\r\n",
        "            conv4_2, 512, 3, 1, activation_fn=None, scope='conv4_3')\r\n",
        "        conv4_3 = tf.nn.relu(conv4_3)\r\n",
        "        conv4_4 = layers.conv2d(\r\n",
        "            conv4_3, 512, 3, 1, activation_fn=None, scope='conv4_4')\r\n",
        "        conv4_4 = tf.nn.relu(conv4_4)\r\n",
        "        conv5_1 = layers.conv2d(\r\n",
        "            conv4_4, 512, 3, 1, activation_fn=None, scope='conv5_1')\r\n",
        "        conv5_1 = tf.nn.relu(conv5_1)\r\n",
        "        conv5_2_CPM = layers.conv2d(\r\n",
        "            conv5_1, 128, 3, 1, activation_fn=None, scope='conv5_2_CPM')\r\n",
        "        conv5_2_CPM = tf.nn.relu(conv5_2_CPM)\r\n",
        "        conv6_1_CPM = layers.conv2d(\r\n",
        "            conv5_2_CPM, 512, 1, 1, activation_fn=None, scope='conv6_1_CPM')\r\n",
        "        conv6_1_CPM = tf.nn.relu(conv6_1_CPM)\r\n",
        "        conv6_2_CPM = layers.conv2d(\r\n",
        "            conv6_1_CPM, 1, 1, 1, activation_fn=None, scope='conv6_2_CPM')\r\n",
        "        concat_stage2 = tf.concat([conv6_2_CPM, conv5_2_CPM], 3)\r\n",
        "        Mconv1_stage2 = layers.conv2d(\r\n",
        "            concat_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage2')\r\n",
        "        Mconv1_stage2 = tf.nn.relu(Mconv1_stage2)\r\n",
        "        Mconv2_stage2 = layers.conv2d(\r\n",
        "            Mconv1_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage2')\r\n",
        "        Mconv2_stage2 = tf.nn.relu(Mconv2_stage2)\r\n",
        "        Mconv3_stage2 = layers.conv2d(\r\n",
        "            Mconv2_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage2')\r\n",
        "        Mconv3_stage2 = tf.nn.relu(Mconv3_stage2)\r\n",
        "        Mconv4_stage2 = layers.conv2d(\r\n",
        "            Mconv3_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage2')\r\n",
        "        Mconv4_stage2 = tf.nn.relu(Mconv4_stage2)\r\n",
        "        Mconv5_stage2 = layers.conv2d(\r\n",
        "            Mconv4_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage2')\r\n",
        "        Mconv5_stage2 = tf.nn.relu(Mconv5_stage2)\r\n",
        "        Mconv6_stage2 = layers.conv2d(\r\n",
        "            Mconv5_stage2, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage2')\r\n",
        "        Mconv6_stage2 = tf.nn.relu(Mconv6_stage2)\r\n",
        "        Mconv7_stage2 = layers.conv2d(\r\n",
        "            Mconv6_stage2, 1, 1, 1, activation_fn=None, scope='Mconv7_stage2')\r\n",
        "        concat_stage3 = tf.concat([Mconv7_stage2, conv5_2_CPM], 3)\r\n",
        "        Mconv1_stage3 = layers.conv2d(\r\n",
        "            concat_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage3')\r\n",
        "        Mconv1_stage3 = tf.nn.relu(Mconv1_stage3)\r\n",
        "        Mconv2_stage3 = layers.conv2d(\r\n",
        "            Mconv1_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage3')\r\n",
        "        Mconv2_stage3 = tf.nn.relu(Mconv2_stage3)\r\n",
        "        Mconv3_stage3 = layers.conv2d(\r\n",
        "            Mconv2_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage3')\r\n",
        "        Mconv3_stage3 = tf.nn.relu(Mconv3_stage3)\r\n",
        "        Mconv4_stage3 = layers.conv2d(\r\n",
        "            Mconv3_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage3')\r\n",
        "        Mconv4_stage3 = tf.nn.relu(Mconv4_stage3)\r\n",
        "        Mconv5_stage3 = layers.conv2d(\r\n",
        "            Mconv4_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage3')\r\n",
        "        Mconv5_stage3 = tf.nn.relu(Mconv5_stage3)\r\n",
        "        Mconv6_stage3 = layers.conv2d(\r\n",
        "            Mconv5_stage3, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage3')\r\n",
        "        Mconv6_stage3 = tf.nn.relu(Mconv6_stage3)\r\n",
        "        Mconv7_stage3 = layers.conv2d(\r\n",
        "            Mconv6_stage3, 1, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv7_stage3')\r\n",
        "        concat_stage4 = tf.concat([Mconv7_stage3, conv5_2_CPM], 3)\r\n",
        "        Mconv1_stage4 = layers.conv2d(\r\n",
        "            concat_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage4')\r\n",
        "        Mconv1_stage4 = tf.nn.relu(Mconv1_stage4)\r\n",
        "        Mconv2_stage4 = layers.conv2d(\r\n",
        "            Mconv1_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage4')\r\n",
        "        Mconv2_stage4 = tf.nn.relu(Mconv2_stage4)\r\n",
        "        Mconv3_stage4 = layers.conv2d(\r\n",
        "            Mconv2_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage4')\r\n",
        "        Mconv3_stage4 = tf.nn.relu(Mconv3_stage4)\r\n",
        "        Mconv4_stage4 = layers.conv2d(\r\n",
        "            Mconv3_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage4')\r\n",
        "        Mconv4_stage4 = tf.nn.relu(Mconv4_stage4)\r\n",
        "        Mconv5_stage4 = layers.conv2d(\r\n",
        "            Mconv4_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage4')\r\n",
        "        Mconv5_stage4 = tf.nn.relu(Mconv5_stage4)\r\n",
        "        Mconv6_stage4 = layers.conv2d(\r\n",
        "            Mconv5_stage4, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage4')\r\n",
        "        Mconv6_stage4 = tf.nn.relu(Mconv6_stage4)\r\n",
        "        Mconv7_stage4 = layers.conv2d(\r\n",
        "            Mconv6_stage4, 1, 1, 1, activation_fn=None, scope='Mconv7_stage4')\r\n",
        "    return Mconv7_stage4\r\n",
        "\r\n",
        "\r\n",
        "def _argmax_2d(tensor):\r\n",
        "    \"\"\"\r\n",
        "    Compute argmax on the 2nd and 3d dimensions of the tensor.\r\n",
        "    e.g. given an input tensor of size N x K x K x C, then it computes the (x,y) coordinates for\r\n",
        "    each of the N images and C channels, corresponding to the max for that image and channel.\r\n",
        "    :param tensor: image of size N x K x K x C\r\n",
        "    :return: argmax in the format N x 2 x C (where C corresponds to NUM_JOINTS)\r\n",
        "    \"\"\"\r\n",
        "    # get size\r\n",
        "    shape = tensor.get_shape().as_list()[1]\r\n",
        "    n_channels = tf.shape(tensor)[-1]\r\n",
        "\r\n",
        "    # process each channel\r\n",
        "    linearised_channel = tf.reshape(tensor, [-1, shape * shape, n_channels])\r\n",
        "    best_channel = tf.argmax(linearised_channel, axis=1)\r\n",
        "\r\n",
        "    idx_y = tf.expand_dims(tf.floordiv(best_channel, shape), axis=1)\r\n",
        "    idx_x = tf.expand_dims(tf.mod(best_channel, shape), axis=1)\r\n",
        "    argmax_channels = tf.concat([idx_x, idx_y], axis=1)\r\n",
        "    return argmax_channels\r\n",
        "\r\n",
        "\r\n",
        "def _process_stage(heat_maps, hm_size):\r\n",
        "    \"\"\"\r\n",
        "    For each heat-map identify joint position and likelihood\r\n",
        "    :param heat_maps: input heat-maps\r\n",
        "    :param hm_size: size in which to return the coordinates\r\n",
        "    :return: 2d joints (BATCH_SIZE x 14 x 2)\r\n",
        "             likelihood for each joint (BATCH_SIZE x 14)\r\n",
        "    \"\"\"\r\n",
        "    rescaled = tf.image.resize_images(heat_maps[:, :, :, :-1], [hm_size, hm_size])\r\n",
        "    uncertainty = tf.reduce_max(tf.reduce_mean(rescaled, axis=1), axis=1)\r\n",
        "    return _argmax_2d(rescaled), uncertainty\r\n",
        "\r\n",
        "\r\n",
        "def inference_pose(image, center_map, hm_size, stage=6):\r\n",
        "    with tf.variable_scope('PoseNet'):\r\n",
        "        pool_center_lower = layers.avg_pool2d(center_map, 9, 8, padding='SAME')\r\n",
        "        conv1_1 = layers.conv2d(\r\n",
        "            image, 64, 3, 1, activation_fn=None, scope='conv1_1')\r\n",
        "        conv1_1 = tf.nn.relu(conv1_1)\r\n",
        "        conv1_2 = layers.conv2d(\r\n",
        "            conv1_1, 64, 3, 1, activation_fn=None, scope='conv1_2')\r\n",
        "        conv1_2 = tf.nn.relu(conv1_2)\r\n",
        "        pool1_stage1 = layers.max_pool2d(conv1_2, 2, 2)\r\n",
        "        conv2_1 = layers.conv2d(pool1_stage1, 128, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv2_1')\r\n",
        "        conv2_1 = tf.nn.relu(conv2_1)\r\n",
        "        conv2_2 = layers.conv2d(\r\n",
        "            conv2_1, 128, 3, 1, activation_fn=None, scope='conv2_2')\r\n",
        "        conv2_2 = tf.nn.relu(conv2_2)\r\n",
        "        pool2_stage1 = layers.max_pool2d(conv2_2, 2, 2)\r\n",
        "        conv3_1 = layers.conv2d(pool2_stage1, 256, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv3_1')\r\n",
        "        conv3_1 = tf.nn.relu(conv3_1)\r\n",
        "        conv3_2 = layers.conv2d(\r\n",
        "            conv3_1, 256, 3, 1, activation_fn=None, scope='conv3_2')\r\n",
        "        conv3_2 = tf.nn.relu(conv3_2)\r\n",
        "        conv3_3 = layers.conv2d(\r\n",
        "            conv3_2, 256, 3, 1, activation_fn=None, scope='conv3_3')\r\n",
        "        conv3_3 = tf.nn.relu(conv3_3)\r\n",
        "        conv3_4 = layers.conv2d(\r\n",
        "            conv3_3, 256, 3, 1, activation_fn=None, scope='conv3_4')\r\n",
        "        conv3_4 = tf.nn.relu(conv3_4)\r\n",
        "        pool3_stage1 = layers.max_pool2d(conv3_4, 2, 2)\r\n",
        "        conv4_1 = layers.conv2d(pool3_stage1, 512, 3, 1,\r\n",
        "                                activation_fn=None, scope='conv4_1')\r\n",
        "        conv4_1 = tf.nn.relu(conv4_1)\r\n",
        "        conv4_2 = layers.conv2d(\r\n",
        "            conv4_1, 512, 3, 1, activation_fn=None, scope='conv4_2')\r\n",
        "        conv4_2 = tf.nn.relu(conv4_2)\r\n",
        "        conv4_3_CPM = layers.conv2d(\r\n",
        "            conv4_2, 256, 3, 1, activation_fn=None, scope='conv4_3_CPM')\r\n",
        "        conv4_3_CPM = tf.nn.relu(conv4_3_CPM)\r\n",
        "        conv4_4_CPM = layers.conv2d(\r\n",
        "            conv4_3_CPM, 256, 3, 1, activation_fn=None, scope='conv4_4_CPM')\r\n",
        "        conv4_4_CPM = tf.nn.relu(conv4_4_CPM)\r\n",
        "        conv4_5_CPM = layers.conv2d(\r\n",
        "            conv4_4_CPM, 256, 3, 1, activation_fn=None, scope='conv4_5_CPM')\r\n",
        "        conv4_5_CPM = tf.nn.relu(conv4_5_CPM)\r\n",
        "        conv4_6_CPM = layers.conv2d(\r\n",
        "            conv4_5_CPM, 256, 3, 1, activation_fn=None, scope='conv4_6_CPM')\r\n",
        "        conv4_6_CPM = tf.nn.relu(conv4_6_CPM)\r\n",
        "        conv4_7_CPM = layers.conv2d(\r\n",
        "            conv4_6_CPM, 128, 3, 1, activation_fn=None, scope='conv4_7_CPM')\r\n",
        "        conv4_7_CPM = tf.nn.relu(conv4_7_CPM)\r\n",
        "        conv5_1_CPM = layers.conv2d(\r\n",
        "            conv4_7_CPM, 512, 1, 1, activation_fn=None, scope='conv5_1_CPM')\r\n",
        "        conv5_1_CPM = tf.nn.relu(conv5_1_CPM)\r\n",
        "        conv5_2_CPM = layers.conv2d(\r\n",
        "            conv5_1_CPM, 15, 1, 1, activation_fn=None, scope='conv5_2_CPM')\r\n",
        "        concat_stage2 = tf.concat(\r\n",
        "            [conv5_2_CPM, conv4_7_CPM, pool_center_lower], 3)\r\n",
        "        Mconv1_stage2 = layers.conv2d(\r\n",
        "            concat_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage2')\r\n",
        "        Mconv1_stage2 = tf.nn.relu(Mconv1_stage2)\r\n",
        "        Mconv2_stage2 = layers.conv2d(\r\n",
        "            Mconv1_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage2')\r\n",
        "        Mconv2_stage2 = tf.nn.relu(Mconv2_stage2)\r\n",
        "        Mconv3_stage2 = layers.conv2d(\r\n",
        "            Mconv2_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage2')\r\n",
        "        Mconv3_stage2 = tf.nn.relu(Mconv3_stage2)\r\n",
        "        Mconv4_stage2 = layers.conv2d(\r\n",
        "            Mconv3_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage2')\r\n",
        "        Mconv4_stage2 = tf.nn.relu(Mconv4_stage2)\r\n",
        "        Mconv5_stage2 = layers.conv2d(\r\n",
        "            Mconv4_stage2, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage2')\r\n",
        "        Mconv5_stage2 = tf.nn.relu(Mconv5_stage2)\r\n",
        "        Mconv6_stage2 = layers.conv2d(\r\n",
        "            Mconv5_stage2, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage2')\r\n",
        "        Mconv6_stage2 = tf.nn.relu(Mconv6_stage2)\r\n",
        "        Mconv7_stage2 = layers.conv2d(\r\n",
        "            Mconv6_stage2, 15, 1, 1, activation_fn=None, scope='Mconv7_stage2')\r\n",
        "        if stage == 2:\r\n",
        "            return _process_stage(Mconv7_stage2, hm_size)\r\n",
        "\r\n",
        "        concat_stage3 = tf.concat(\r\n",
        "            [Mconv7_stage2, conv4_7_CPM, pool_center_lower], 3)\r\n",
        "        Mconv1_stage3 = layers.conv2d(\r\n",
        "            concat_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage3')\r\n",
        "        Mconv1_stage3 = tf.nn.relu(Mconv1_stage3)\r\n",
        "        Mconv2_stage3 = layers.conv2d(\r\n",
        "            Mconv1_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage3')\r\n",
        "        Mconv2_stage3 = tf.nn.relu(Mconv2_stage3)\r\n",
        "        Mconv3_stage3 = layers.conv2d(\r\n",
        "            Mconv2_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage3')\r\n",
        "        Mconv3_stage3 = tf.nn.relu(Mconv3_stage3)\r\n",
        "        Mconv4_stage3 = layers.conv2d(\r\n",
        "            Mconv3_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage3')\r\n",
        "        Mconv4_stage3 = tf.nn.relu(Mconv4_stage3)\r\n",
        "        Mconv5_stage3 = layers.conv2d(\r\n",
        "            Mconv4_stage3, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage3')\r\n",
        "        Mconv5_stage3 = tf.nn.relu(Mconv5_stage3)\r\n",
        "        Mconv6_stage3 = layers.conv2d(\r\n",
        "            Mconv5_stage3, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage3')\r\n",
        "        Mconv6_stage3 = tf.nn.relu(Mconv6_stage3)\r\n",
        "        Mconv7_stage3 = layers.conv2d(\r\n",
        "            Mconv6_stage3, 15, 1, 1, activation_fn=None, scope='Mconv7_stage3')\r\n",
        "        if stage == 3:\r\n",
        "            return _process_stage(Mconv7_stage3, hm_size)\r\n",
        "\r\n",
        "        concat_stage4 = tf.concat(\r\n",
        "            [Mconv7_stage3, conv4_7_CPM, pool_center_lower], 3)\r\n",
        "        Mconv1_stage4 = layers.conv2d(\r\n",
        "            concat_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage4')\r\n",
        "        Mconv1_stage4 = tf.nn.relu(Mconv1_stage4)\r\n",
        "        Mconv2_stage4 = layers.conv2d(\r\n",
        "            Mconv1_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage4')\r\n",
        "        Mconv2_stage4 = tf.nn.relu(Mconv2_stage4)\r\n",
        "        Mconv3_stage4 = layers.conv2d(\r\n",
        "            Mconv2_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage4')\r\n",
        "        Mconv3_stage4 = tf.nn.relu(Mconv3_stage4)\r\n",
        "        Mconv4_stage4 = layers.conv2d(\r\n",
        "            Mconv3_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage4')\r\n",
        "        Mconv4_stage4 = tf.nn.relu(Mconv4_stage4)\r\n",
        "        Mconv5_stage4 = layers.conv2d(\r\n",
        "            Mconv4_stage4, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage4')\r\n",
        "        Mconv5_stage4 = tf.nn.relu(Mconv5_stage4)\r\n",
        "        Mconv6_stage4 = layers.conv2d(\r\n",
        "            Mconv5_stage4, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage4')\r\n",
        "        Mconv6_stage4 = tf.nn.relu(Mconv6_stage4)\r\n",
        "        Mconv7_stage4 = layers.conv2d(\r\n",
        "            Mconv6_stage4, 15, 1, 1, activation_fn=None, scope='Mconv7_stage4')\r\n",
        "        if stage == 4:\r\n",
        "            return _process_stage(Mconv7_stage4, hm_size)\r\n",
        "\r\n",
        "        concat_stage5 = tf.concat(\r\n",
        "            [Mconv7_stage4, conv4_7_CPM, pool_center_lower], 3)\r\n",
        "        Mconv1_stage5 = layers.conv2d(\r\n",
        "            concat_stage5, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage5')\r\n",
        "        Mconv1_stage5 = tf.nn.relu(Mconv1_stage5)\r\n",
        "        Mconv2_stage5 = layers.conv2d(\r\n",
        "            Mconv1_stage5, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage5')\r\n",
        "        Mconv2_stage5 = tf.nn.relu(Mconv2_stage5)\r\n",
        "        Mconv3_stage5 = layers.conv2d(\r\n",
        "            Mconv2_stage5, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage5')\r\n",
        "        Mconv3_stage5 = tf.nn.relu(Mconv3_stage5)\r\n",
        "        Mconv4_stage5 = layers.conv2d(\r\n",
        "            Mconv3_stage5, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage5')\r\n",
        "        Mconv4_stage5 = tf.nn.relu(Mconv4_stage5)\r\n",
        "        Mconv5_stage5 = layers.conv2d(\r\n",
        "            Mconv4_stage5, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage5')\r\n",
        "        Mconv5_stage5 = tf.nn.relu(Mconv5_stage5)\r\n",
        "        Mconv6_stage5 = layers.conv2d(\r\n",
        "            Mconv5_stage5, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage5')\r\n",
        "        Mconv6_stage5 = tf.nn.relu(Mconv6_stage5)\r\n",
        "        Mconv7_stage5 = layers.conv2d(\r\n",
        "            Mconv6_stage5, 15, 1, 1, activation_fn=None, scope='Mconv7_stage5')\r\n",
        "        if stage == 5:\r\n",
        "            return _process_stage(Mconv7_stage5, hm_size)\r\n",
        "\r\n",
        "        concat_stage6 = tf.concat(\r\n",
        "            [Mconv7_stage5, conv4_7_CPM, pool_center_lower], 3)\r\n",
        "        Mconv1_stage6 = layers.conv2d(\r\n",
        "            concat_stage6, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv1_stage6')\r\n",
        "        Mconv1_stage6 = tf.nn.relu(Mconv1_stage6)\r\n",
        "        Mconv2_stage6 = layers.conv2d(\r\n",
        "            Mconv1_stage6, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv2_stage6')\r\n",
        "        Mconv2_stage6 = tf.nn.relu(Mconv2_stage6)\r\n",
        "        Mconv3_stage6 = layers.conv2d(\r\n",
        "            Mconv2_stage6, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv3_stage6')\r\n",
        "        Mconv3_stage6 = tf.nn.relu(Mconv3_stage6)\r\n",
        "        Mconv4_stage6 = layers.conv2d(\r\n",
        "            Mconv3_stage6, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv4_stage6')\r\n",
        "        Mconv4_stage6 = tf.nn.relu(Mconv4_stage6)\r\n",
        "        Mconv5_stage6 = layers.conv2d(\r\n",
        "            Mconv4_stage6, 128, 7, 1, activation_fn=None,\r\n",
        "            scope='Mconv5_stage6')\r\n",
        "        Mconv5_stage6 = tf.nn.relu(Mconv5_stage6)\r\n",
        "        Mconv6_stage6 = layers.conv2d(\r\n",
        "            Mconv5_stage6, 128, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv6_stage6')\r\n",
        "        Mconv6_stage6 = tf.nn.relu(Mconv6_stage6)\r\n",
        "        Mconv7_stage6 = layers.conv2d(\r\n",
        "            Mconv6_stage6, 15, 1, 1, activation_fn=None,\r\n",
        "            scope='Mconv7_stage6')\r\n",
        "        return _process_stage(Mconv7_stage6, hm_size)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t9cT26drEN-"
      },
      "source": [
        "draw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aw83M37rDjA"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "#from .config import JOINT_DRAW_SIZE\r\n",
        "#from .config import LIMB_DRAW_SIZE\r\n",
        "#from .config import NORMALISATION_COEFFICIENT\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "\r\n",
        "__all__ = [\r\n",
        "    'draw_limbs',\r\n",
        "    'plot_pose'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def draw_limbs(image, pose_2d, visible):\r\n",
        "    \"\"\"Draw the 2D pose without the occluded/not visible joints.\"\"\"\r\n",
        "\r\n",
        "    _COLORS = [\r\n",
        "        [0, 0, 255], [0, 170, 255], [0, 255, 170], [0, 255, 0],\r\n",
        "        [170, 255, 0], [255, 170, 0], [255, 0, 0], [255, 0, 170],\r\n",
        "        [170, 0, 255]\r\n",
        "    ]\r\n",
        "    _LIMBS = np.array([0, 1, 2, 3, 3, 4, 5, 6, 6, 7, 8, 9,\r\n",
        "                       9, 10, 11, 12, 12, 13]).reshape((-1, 2))\r\n",
        "\r\n",
        "    _NORMALISATION_FACTOR = int(math.floor(math.sqrt(image.shape[0] * image.shape[1] / NORMALISATION_COEFFICIENT)))\r\n",
        "\r\n",
        "    for oid in range(pose_2d.shape[0]):\r\n",
        "        for lid, (p0, p1) in enumerate(_LIMBS):\r\n",
        "            if not (visible[oid][p0] and visible[oid][p1]):\r\n",
        "                continue\r\n",
        "            y0, x0 = pose_2d[oid][p0]\r\n",
        "            y1, x1 = pose_2d[oid][p1]\r\n",
        "            cv2.circle(image, (x0, y0), JOINT_DRAW_SIZE *_NORMALISATION_FACTOR , _COLORS[lid], -1)\r\n",
        "            cv2.circle(image, (x1, y1), JOINT_DRAW_SIZE*_NORMALISATION_FACTOR , _COLORS[lid], -1)\r\n",
        "            cv2.line(image, (x0, y0), (x1, y1),\r\n",
        "                     _COLORS[lid], LIMB_DRAW_SIZE*_NORMALISATION_FACTOR , 16)\r\n",
        "\r\n",
        "\r\n",
        "def plot_pose(pose):\r\n",
        "    \"\"\"Plot the 3D pose showing the joint connections.\"\"\"\r\n",
        "    import mpl_toolkits.mplot3d.axes3d as p3\r\n",
        "\r\n",
        "    _CONNECTION = [\r\n",
        "        [0, 1], [1, 2], [2, 3], [0, 4], [4, 5], [5, 6], [0, 7], [7, 8],\r\n",
        "        [8, 9], [9, 10], [8, 11], [11, 12], [12, 13], [8, 14], [14, 15],\r\n",
        "        [15, 16]]\r\n",
        "\r\n",
        "    def joint_color(j):\r\n",
        "        \"\"\"\r\n",
        "        TODO: 'j' shadows name 'j' from outer scope\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        colors = [(0, 0, 0), (255, 0, 255), (0, 0, 255),\r\n",
        "                  (0, 255, 255), (255, 0, 0), (0, 255, 0)]\r\n",
        "        _c = 0\r\n",
        "        if j in range(1, 4):\r\n",
        "            _c = 1\r\n",
        "        if j in range(4, 7):\r\n",
        "            _c = 2\r\n",
        "        if j in range(9, 11):\r\n",
        "            _c = 3\r\n",
        "        if j in range(11, 14):\r\n",
        "            _c = 4\r\n",
        "        if j in range(14, 17):\r\n",
        "            _c = 5\r\n",
        "        return colors[_c]\r\n",
        "\r\n",
        "    assert (pose.ndim == 2)\r\n",
        "    assert (pose.shape[0] == 3)\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.gca(projection='3d')\r\n",
        "    for c in _CONNECTION:\r\n",
        "        col = '#%02x%02x%02x' % joint_color(c[0])\r\n",
        "        ax.plot([pose[0, c[0]], pose[0, c[1]]],\r\n",
        "                [pose[1, c[0]], pose[1, c[1]]],\r\n",
        "                [pose[2, c[0]], pose[2, c[1]]], c=col)\r\n",
        "    for j in range(pose.shape[1]):\r\n",
        "        col = '#%02x%02x%02x' % joint_color(j)\r\n",
        "        ax.scatter(pose[0, j], pose[1, j], pose[2, j],\r\n",
        "                   c=col, marker='o', edgecolor=col)\r\n",
        "    smallest = pose.min()\r\n",
        "    largest = pose.max()\r\n",
        "    ax.set_xlim3d(smallest, largest)\r\n",
        "    ax.set_ylim3d(smallest, largest)\r\n",
        "    ax.set_zlim3d(smallest, largest)\r\n",
        "\r\n",
        "    return fig\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8vWWdB9vaBo"
      },
      "source": [
        "upright_fast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR335g4vviOa"
      },
      "source": [
        "import numpy as np\r\n",
        "import scipy\r\n",
        "\r\n",
        "__all__ = [\r\n",
        "    'upgrade_r',\r\n",
        "    'update_cam',\r\n",
        "    'estimate_a_and_r_with_res',\r\n",
        "    'estimate_a_and_r_with_res_weights',\r\n",
        "    'pick_e'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def upgrade_r(r):\r\n",
        "    \"\"\"Upgrades complex parameterisation of planar rotation to tensor containing\r\n",
        "    per frame 3x3 rotation matrices\"\"\"\r\n",
        "    newr = np.zeros((3, 3))\r\n",
        "    newr[:2, 0] = r\r\n",
        "    newr[2, 2] = 1\r\n",
        "    newr[1::-1, 1] = r\r\n",
        "    newr[0, 1] *= -1\r\n",
        "    return newr\r\n",
        "\r\n",
        "\r\n",
        "def update_cam(cam):\r\n",
        "    new_cam = cam[[0, 2, 1]].copy()\r\n",
        "    new_cam = new_cam[:, [0, 2, 1]]\r\n",
        "    return new_cam\r\n",
        "\r\n",
        "\r\n",
        "def estimate_a_and_r_with_res(\r\n",
        "        w, e, s0, camera_r, Lambda, check, a, weights, res, proj_e,\r\n",
        "        residue, Ps, depth_reg, scale_prior):\r\n",
        "    \"\"\"\r\n",
        "    TODO: Missing the following parameters in docstring:\r\n",
        "        - w, e, s0, camera_r, Lambda, check, a, res, proj_e, depth_reg,\r\n",
        "          scale_prior\r\n",
        "\r\n",
        "    TODO: The following parameters are not used:\r\n",
        "        - s0, weights\r\n",
        "\r\n",
        "    So local optima are a problem in general.\r\n",
        "    However:\r\n",
        "\r\n",
        "        1. This problem is convex in a but not in r, and\r\n",
        "\r\n",
        "        2. each frame can be solved independently.\r\n",
        "\r\n",
        "    So for each frame, we can do a grid search in r and take the globally\r\n",
        "    optimal solution.\r\n",
        "\r\n",
        "    In practice, we just brute force over 100 different estimates of r, and\r\n",
        "    take the best pair (r,a*(r)) where a*(r) is the optimal minimiser of a\r\n",
        "    given r.\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "\r\n",
        "        w is a 3d measurement matrix of form frames*2*points\r\n",
        "\r\n",
        "        e is a 3d set of basis vectors of from basis*3*points\r\n",
        "\r\n",
        "        s0 is the 3d rest shape of form 3*points\r\n",
        "\r\n",
        "        Lambda are the regularisor coefficients on the coefficients of the\r\n",
        "        weights typically generated using PPCA\r\n",
        "\r\n",
        "        interval is how far round the circle we should check for break points\r\n",
        "        we check every interval*2*pi radians\r\n",
        "\r\n",
        "    Returns:\r\n",
        "\r\n",
        "        a (basis coefficients) and r (representation of rotations as a complex\r\n",
        "        number)\r\n",
        "    \"\"\"\r\n",
        "    frames = w.shape[0]\r\n",
        "    points = w.shape[2]\r\n",
        "    basis = e.shape[0]\r\n",
        "    r = np.empty(2)\r\n",
        "    Ps_reshape = Ps.reshape(2 * points)\r\n",
        "    w_reshape = w.reshape((frames, points * 2))\r\n",
        "\r\n",
        "    for i in range(check.size):\r\n",
        "        c = check[i]\r\n",
        "        r[0] = np.cos(c)\r\n",
        "        r[1] = np.sin(c)\r\n",
        "        grot = camera_r.dot(upgrade_r(r))\r\n",
        "        rot = grot[:2]\r\n",
        "        res[:, :points * 2] = w_reshape\r\n",
        "        res[:, :points * 2] -= Ps_reshape\r\n",
        "        proj_e[:, :2 * points] = rot.dot(e).transpose(1, 0, 2).reshape(\r\n",
        "            e.shape[0], 2 * points)\r\n",
        "\r\n",
        "        if Lambda.size != 0:\r\n",
        "            proj_e[:, 2 * points:2 * points + basis] = np.diag(Lambda[:Lambda.shape[0] - 1])\r\n",
        "            res[:, 2 * points:].fill(0)\r\n",
        "            res[:, :points * 2] *= Lambda[Lambda.shape[0] - 1]\r\n",
        "            proj_e[:, :points * 2] *= Lambda[Lambda.shape[0] - 1]\r\n",
        "            # depth regularizer not used\r\n",
        "            proj_e[:, 2 * points + basis:] = ((Lambda[Lambda.shape[0] - 1] *\r\n",
        "                                               depth_reg) * grot[2]).dot(e)\r\n",
        "            # we let the person change scale\r\n",
        "            res[:, 2 * points] = scale_prior\r\n",
        "\r\n",
        "        \"\"\"\r\n",
        "        TODO: PLEASE REVIEW THE FOLLOWING CODE....\r\n",
        "        overwrite_a and overwrite_b ARE UNEXPECTED ARGUMENTS OF\r\n",
        "        scipy.linalg.lstsq\r\n",
        "        \"\"\"\r\n",
        "        a[i], residue[i], _, _ = scipy.linalg.lstsq(\r\n",
        "            proj_e.T, res.T, overwrite_a=True, overwrite_b=True)\r\n",
        "\r\n",
        "    # find and return best coresponding solution\r\n",
        "    best = np.argmin(residue, 0)\r\n",
        "    assert (best.shape[0] == frames)\r\n",
        "    theta = check[best]\r\n",
        "    index = (best, np.arange(frames))\r\n",
        "    aa = a.transpose(0, 2, 1)[index]\r\n",
        "    retres = residue[index]\r\n",
        "    r = np.empty((2, frames))\r\n",
        "    r[0] = np.sin(theta)\r\n",
        "    r[1] = np.cos(theta)\r\n",
        "    return aa, r, retres\r\n",
        "\r\n",
        "\r\n",
        "def estimate_a_and_r_with_res_weights(\r\n",
        "        w, e, s0, camera_r, Lambda, check, a, weights, res, proj_e,\r\n",
        "        residue, Ps, depth_reg, scale_prior):\r\n",
        "    \"\"\"\r\n",
        "    TODO: Missing the following parameters in docstring:\r\n",
        "     - w, e, s0, camera)r, Lambda, check, a, res, proj_e, residue,\r\n",
        "     Ps, depth_reg, scale_prior\r\n",
        "\r\n",
        "    So local optima are a problem in general.\r\n",
        "    However:\r\n",
        "\r\n",
        "        1. This problem is convex in a but not in r, and\r\n",
        "\r\n",
        "        2. each frame can be solved independently.\r\n",
        "\r\n",
        "    So for each frame, we can do a grid search in r and take the globally\r\n",
        "    optimal solution.\r\n",
        "\r\n",
        "    In practice, we just brute force over 100 different estimates of r, and\r\n",
        "    take\r\n",
        "    the best pair (r,a*(r)) where a*(r) is the optimal minimiser of a given r.\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "\r\n",
        "        w is a 3d measurement matrix of form frames*2*points\r\n",
        "\r\n",
        "        e is a 3d set of basis vectors of from basis*3*points\r\n",
        "\r\n",
        "        s0 is the 3d rest shape of form 3*points\r\n",
        "\r\n",
        "        Lambda are the regularisor coefficients on the coefficients of the\r\n",
        "        weights\r\n",
        "        typically generated using PPCA\r\n",
        "\r\n",
        "        interval is how far round the circle we should check for break points\r\n",
        "        we check every interval*2*pi radians\r\n",
        "\r\n",
        "    Returns:\r\n",
        "\r\n",
        "        a (basis coefficients) and r (representation of rotations as a complex\r\n",
        "        number)\r\n",
        "    \"\"\"\r\n",
        "    frames = w.shape[0]\r\n",
        "    points = w.shape[2]\r\n",
        "    basis = e.shape[0]\r\n",
        "    r = np.empty(2)\r\n",
        "    Ps_reshape = Ps.reshape(2 * points)\r\n",
        "    w_reshape = w.reshape((frames, points * 2))\r\n",
        "    p_copy = np.empty_like(proj_e)\r\n",
        "\r\n",
        "    for i in range(check.size):\r\n",
        "        c = check[i]\r\n",
        "        r[0] = np.sin(c)\r\n",
        "        r[1] = np.cos(c)\r\n",
        "        grot = camera_r.dot(upgrade_r(r).T)\r\n",
        "        rot = grot[:2]\r\n",
        "        rot.dot(s0, Ps)  # TODO: remove?\r\n",
        "        res[:, :points * 2] = w_reshape\r\n",
        "        res[:, :points * 2] -= Ps_reshape\r\n",
        "        proj_e[:, :2 * points] = rot.dot(e).transpose(1, 0, 2).reshape(\r\n",
        "            e.shape[0], 2 * points)\r\n",
        "\r\n",
        "        if Lambda.size != 0:\r\n",
        "            proj_e[:, 2 * points:2 * points + basis] = np.diag(Lambda[:Lambda.shape[0] - 1])\r\n",
        "            res[:, 2 * points:].fill(0)\r\n",
        "            res[:, :points * 2] *= Lambda[Lambda.shape[0] - 1]\r\n",
        "            proj_e[:, :points * 2] *= Lambda[Lambda.shape[0] - 1]\r\n",
        "            proj_e[:, 2 * points + basis:] = ((Lambda[Lambda.shape[0] - 1] *\r\n",
        "                                               depth_reg) * grot[2]).dot(e)\r\n",
        "            res[:, 2 * points] = scale_prior\r\n",
        "        if weights.size != 0:\r\n",
        "            res[:, :points * 2] *= weights\r\n",
        "        for j in range(frames):\r\n",
        "            p_copy[:] = proj_e\r\n",
        "            p_copy[:, :points * 2] *= weights[j]\r\n",
        "            a[i, :, j], comp_residual, _, _ = np.linalg.lstsq(\r\n",
        "                p_copy.T, res[j].T)\r\n",
        "            if not comp_residual:\r\n",
        "                # equations are over-determined\r\n",
        "                residue[i, j] = 1e-5\r\n",
        "            else:\r\n",
        "                residue[i, j] = comp_residual\r\n",
        "    # find and return best coresponding solution\r\n",
        "    best = np.argmin(residue, 0)\r\n",
        "    index = (best, np.arange(frames))\r\n",
        "    theta = check[best]\r\n",
        "    aa = a.transpose(0, 2, 1)[index]\r\n",
        "    retres = residue[index]\r\n",
        "    r = np.empty((2, frames))\r\n",
        "    r[0] = np.sin(theta)\r\n",
        "    r[1] = np.cos(theta)\r\n",
        "    return aa, r, retres\r\n",
        "\r\n",
        "\r\n",
        "def pick_e(w, e, s0, camera_r=None, Lambda=None,\r\n",
        "           weights=None, scale_prior=-0.0014, interval=0.01, depth_reg=0.0325):\r\n",
        "    \"\"\"Brute force over charts from the manifold to find the best one.\r\n",
        "        Returns best chart index and its a and r coefficients\r\n",
        "        Returns assignment, and a and r coefficents\"\"\"\r\n",
        "\r\n",
        "    camera_r = np.asarray([[1, 0, 0], [0, 0, -1], [0, 1, 0]]\r\n",
        "                          ) if camera_r is None else camera_r\r\n",
        "    Lambda = np.ones((0, 0)) if Lambda is None else Lambda\r\n",
        "    weights = np.ones((0, 0, 0)) if weights is None else weights\r\n",
        "\r\n",
        "    charts = e.shape[0]\r\n",
        "    frames = w.shape[0]\r\n",
        "    basis = e.shape[1]\r\n",
        "    points = e.shape[3]\r\n",
        "    assert (s0.shape[0] == charts)\r\n",
        "    r = np.empty((charts, 2, frames))\r\n",
        "    a = np.empty((charts, frames, e.shape[1]))\r\n",
        "    score = np.empty((charts, frames))\r\n",
        "    check = np.arange(0, 1, interval) * 2 * np.pi\r\n",
        "    cache_a = np.empty((check.size, basis, frames))\r\n",
        "    residue = np.empty((check.size, frames))\r\n",
        "\r\n",
        "    if Lambda.size != 0:\r\n",
        "        res = np.zeros((frames, points * 2 + basis + points))\r\n",
        "        proj_e = np.zeros((basis, 2 * points + basis + points))\r\n",
        "    else:\r\n",
        "        res = np.empty((frames, points * 2))\r\n",
        "        proj_e = np.empty((basis, 2 * points))\r\n",
        "    Ps = np.empty((2, points))\r\n",
        "\r\n",
        "    if weights.size == 0:\r\n",
        "        for i in range(charts):\r\n",
        "            if Lambda.size != 0:\r\n",
        "                a[i], r[i], score[i] = estimate_a_and_r_with_res(\r\n",
        "                    w, e[i], s0[i], camera_r,\r\n",
        "                    Lambda[i], check, cache_a, weights,\r\n",
        "                    res, proj_e, residue, Ps,\r\n",
        "                    depth_reg, scale_prior)\r\n",
        "            else:\r\n",
        "                a[i], r[i], score[i] = estimate_a_and_r_with_res(\r\n",
        "                    w, e[i], s0[i], camera_r, Lambda,\r\n",
        "                    check, cache_a, weights,\r\n",
        "                    res, proj_e, residue, Ps,\r\n",
        "                    depth_reg, scale_prior)\r\n",
        "    else:\r\n",
        "        w2 = weights.reshape(weights.shape[0], -1)\r\n",
        "        for i in range(charts):\r\n",
        "            if Lambda.size != 0:\r\n",
        "                a[i], r[i], score[i] = estimate_a_and_r_with_res_weights(\r\n",
        "                    w, e[i], s0[i], camera_r,\r\n",
        "                    Lambda[i], check, cache_a, w2,\r\n",
        "                    res, proj_e, residue, Ps,\r\n",
        "                    depth_reg, scale_prior)\r\n",
        "            else:\r\n",
        "                a[i], r[i], score[i] = estimate_a_and_r_with_res_weights(\r\n",
        "                    w, e[i], s0[i], camera_r, Lambda,\r\n",
        "                    check, cache_a, w2,\r\n",
        "                    res, proj_e, residue, Ps,\r\n",
        "                    depth_reg, scale_prior)\r\n",
        "\r\n",
        "    remaining_dims = 3 * w.shape[2] - e.shape[1]\r\n",
        "    assert (np.all(score > 0))\r\n",
        "    assert (remaining_dims >= 0)\r\n",
        "    # Zero problems in log space due to un-regularised first co-efficient\r\n",
        "    l = Lambda.copy()\r\n",
        "    l[Lambda == 0] = 1\r\n",
        "    llambda = -np.log(l)\r\n",
        "    score /= 2\r\n",
        "    return score, a, r\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ0B6QwqxKMw"
      },
      "source": [
        "process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_aRmwc0xMdN"
      },
      "source": [
        "from __future__ import division\r\n",
        "\r\n",
        "import os\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "#from lifting.utils import config\r\n",
        "import cv2\r\n",
        "import skimage.io\r\n",
        "import skimage.transform\r\n",
        "import scipy.ndimage as ndimage\r\n",
        "import scipy.ndimage.filters as filters\r\n",
        "from itertools import compress\r\n",
        "from scipy.stats import multivariate_normal\r\n",
        "\r\n",
        "__all__ = [\r\n",
        "    'detect_objects_heatmap',\r\n",
        "    'detect_objects_heatmap',\r\n",
        "    'gaussian_kernel',\r\n",
        "    'gaussian_heatmap',\r\n",
        "    'prepare_input_posenet',\r\n",
        "    'detect_parts_heatmaps',\r\n",
        "    'detect_parts_from_likelihoods',\r\n",
        "    'import_json',\r\n",
        "    'generate_labels',\r\n",
        "    'generate_center_map',\r\n",
        "    'rescale',\r\n",
        "    'crop_image'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def detect_objects_heatmap(heatmap):\r\n",
        "    data = 256 * heatmap\r\n",
        "    data_max = filters.maximum_filter(data, 3)\r\n",
        "    maxima = (data == data_max)\r\n",
        "    data_min = filters.minimum_filter(data, 3)\r\n",
        "    diff = ((data_max - data_min) > 0.3)\r\n",
        "    maxima[diff == 0] = 0\r\n",
        "    labeled, num_objects = ndimage.label(maxima)\r\n",
        "    slices = ndimage.find_objects(labeled)\r\n",
        "    objects = np.zeros((num_objects, 2), dtype=np.int32)\r\n",
        "    pidx = 0\r\n",
        "    for (dy, dx) in slices:\r\n",
        "        pos = [(dy.start + dy.stop - 1) // 2, (dx.start + dx.stop - 1) // 2]\r\n",
        "        if heatmap[pos[0], pos[1]] > CENTER_TR:\r\n",
        "            objects[pidx, :] = pos\r\n",
        "            pidx += 1\r\n",
        "    return objects[:pidx]\r\n",
        "\r\n",
        "\r\n",
        "def gaussian_kernel(h, w, sigma_h, sigma_w):\r\n",
        "    yx = np.mgrid[-h // 2:h // 2, -w // 2:w // 2] ** 2\r\n",
        "    return np.exp(-yx[0, :, :] / sigma_h ** 2 - yx[1, :, :] / sigma_w ** 2)\r\n",
        "\r\n",
        "\r\n",
        "def gaussian_heatmap(h, w, pos_x, pos_y, sigma_h=1, sigma_w=1, init=None):\r\n",
        "    \"\"\"\r\n",
        "    Compute the heat-map of size (w x h) with a gaussian distribution fit in\r\n",
        "    position (pos_x, pos_y) and a convariance matix defined by the related\r\n",
        "    sigma values.\r\n",
        "    The resulting heat-map can be summed to a given heat-map init.\r\n",
        "    \"\"\"\r\n",
        "    init = init if init is not None else []\r\n",
        "\r\n",
        "    cov_matrix = np.eye(2) * ([sigma_h**2, sigma_w**2])\r\n",
        "\r\n",
        "    x, y = np.mgrid[0:h, 0:w]\r\n",
        "    pos = np.dstack((x, y))\r\n",
        "    rv = multivariate_normal([pos_x, pos_y], cov_matrix)\r\n",
        "\r\n",
        "    tmp = rv.pdf(pos)\r\n",
        "    hmap = np.multiply(\r\n",
        "        tmp, np.sqrt(np.power(2 * np.pi, 2) * np.linalg.det(cov_matrix))\r\n",
        "    )\r\n",
        "    idx = np.where(hmap.flatten() <= np.exp(-4.6052))\r\n",
        "    hmap.flatten()[idx] = 0\r\n",
        "\r\n",
        "    if np.size(init) == 0:\r\n",
        "        return hmap\r\n",
        "\r\n",
        "    assert (np.shape(init) == hmap.shape)\r\n",
        "    hmap += init\r\n",
        "    idx = np.where(hmap.flatten() > 1)\r\n",
        "    hmap.flatten()[idx] = 1\r\n",
        "    return hmap\r\n",
        "\r\n",
        "\r\n",
        "def prepare_input_posenet(image, objects, size_person, size,\r\n",
        "                          batch_size, sigma=25, border=400):\r\n",
        "    result = np.zeros((batch_size, size[0], size[1], 4))\r\n",
        "    padded_image = np.zeros(\r\n",
        "        (1, size_person[0] + border, size_person[1] + border, 4))\r\n",
        "    padded_image[0, border // 2:-border // 2,\r\n",
        "                 border // 2:-border // 2, :3] = image\r\n",
        "    if objects.shape[0] > batch_size:\r\n",
        "        objects = objects[:batch_size]\r\n",
        "    for oid, (yc, xc) in enumerate(objects):\r\n",
        "        dh, dw = size[0] // 2, size[1] // 2\r\n",
        "        y0, x0, y1, x1 = np.array(\r\n",
        "            [yc - dh, xc - dw, yc + dh, xc + dw]) + border // 2\r\n",
        "        result[oid, :, :, :4] = padded_image[:, y0:y1, x0:x1, :]\r\n",
        "        result[oid, :, :, 3] = gaussian_kernel(size[0], size[1], sigma, sigma)\r\n",
        "    return np.split(result, [3], 3)\r\n",
        "\r\n",
        "\r\n",
        "def detect_parts_heatmaps(heatmaps, centers, size, num_parts=14):\r\n",
        "    \"\"\"\r\n",
        "    Given heat-maps find the position of each joint by means of n argmax\r\n",
        "    function\r\n",
        "    \"\"\"\r\n",
        "    parts = np.zeros((len(centers), num_parts, 2), dtype=np.int32)\r\n",
        "    visible = np.zeros((len(centers), num_parts), dtype=bool)\r\n",
        "    for oid, (yc, xc) in enumerate(centers):\r\n",
        "        part_hmap = skimage.transform.resize(\r\n",
        "            np.clip(heatmaps[oid], -1, 1), size)\r\n",
        "        for pid in range(num_parts):\r\n",
        "            y, x = np.unravel_index(np.argmax(part_hmap[:, :, pid]), size)\r\n",
        "            parts[oid, pid] = y + yc - size[0] // 2, x + xc - size[1] // 2\r\n",
        "            visible[oid, pid] = np.mean(\r\n",
        "                part_hmap[:, :, pid]) > VISIBLE_PART\r\n",
        "    return parts, visible\r\n",
        "\r\n",
        "\r\n",
        "def detect_parts_from_likelihoods(poses, centers, likelihoods, num_parts=14):\r\n",
        "    \"\"\"\r\n",
        "    Given heat-maps find the position of each joint by means of n argmax\r\n",
        "    function\r\n",
        "    \"\"\"\r\n",
        "    if len(centers) > BATCH_SIZE:\r\n",
        "        centers = centers[:BATCH_SIZE]\r\n",
        "    parts = np.zeros((len(centers), num_parts, 2), dtype=np.int32)\r\n",
        "    visible = np.zeros((len(centers), num_parts), dtype=bool)\r\n",
        "    for oid, (yc, xc) in enumerate(centers):\r\n",
        "        for pid in range(num_parts):\r\n",
        "            x, y = poses[oid, :, pid]\r\n",
        "            parts[oid, pid] = y + yc - INPUT_SIZE // 2, x + xc - INPUT_SIZE // 2\r\n",
        "            visible[oid, pid] = likelihoods[oid, pid] > VISIBLE_PART\r\n",
        "    return parts, visible\r\n",
        "\r\n",
        "\r\n",
        "def import_json(path='json/MPI_annotations.json', order='json/MPI_order.npy'):\r\n",
        "    \"\"\"Get the json file containing the dataset.\r\n",
        "    We want the data to be shuffled, however the training has to be repeatable.\r\n",
        "    This means that once shuffled the order has to me mantained.\"\"\"\r\n",
        "    with open(path) as data_file:\r\n",
        "        data_this = json.load(data_file)\r\n",
        "        data_this = np.array(data_this['root'])\r\n",
        "    num_samples = len(data_this)\r\n",
        "\r\n",
        "    if os.path.exists(order):\r\n",
        "        idx = np.load(order)\r\n",
        "    else:\r\n",
        "        idx = np.random.permutation(num_samples).tolist()\r\n",
        "        np.save(order, idx)\r\n",
        "\r\n",
        "    is_not_validation = [not data_this[i]['isValidation']\r\n",
        "                         for i in range(num_samples)]\r\n",
        "    keep_data_idx = list(compress(idx, is_not_validation))\r\n",
        "\r\n",
        "    data = data_this[keep_data_idx]\r\n",
        "    return data, len(keep_data_idx)\r\n",
        "\r\n",
        "\r\n",
        "def generate_labels(image_shape, joint_positions, num_other_people,\r\n",
        "                    joints_other_people, offset):\r\n",
        "    \"\"\"\r\n",
        "    Given as input a set of joint positions and the size of the input image\r\n",
        "    it generates\r\n",
        "    a set of heat-maps of the same size. It generates both heat-maps used as\r\n",
        "    labels for the first stage (label_1st_lower) and for all the other stages\r\n",
        "    (label_lower).\r\n",
        "    \"\"\"\r\n",
        "    _FILTER_JOINTS = np.array([9, 8, 12, 11, 10, 13, 14, 15, 2, 1, 0, 3, 4, 5])\r\n",
        "\r\n",
        "    img_height, img_width, _ = image_shape\r\n",
        "    heat_maps_single_p = np.zeros(\r\n",
        "        (NUM_OUTPUT, INPUT_SIZE, INPUT_SIZE))\r\n",
        "    heat_maps_other_p = np.zeros(\r\n",
        "        (NUM_OUTPUT, INPUT_SIZE, INPUT_SIZE))\r\n",
        "\r\n",
        "    # generate first set of heat-maps\r\n",
        "    for i in range(NUM_JOINTS):\r\n",
        "        # the set of joints can be different fromt the one in the json file\r\n",
        "        curr_joint = joint_positions[_FILTER_JOINTS[i]]\r\n",
        "        skip = (curr_joint[0] < 0 or curr_joint[1] < 0 or\r\n",
        "                curr_joint[0] >= img_width or curr_joint[1] >= img_height)\r\n",
        "        if not skip:\r\n",
        "            heat_maps_single_p[i] = gaussian_heatmap(\r\n",
        "                INPUT_SIZE, INPUT_SIZE,\r\n",
        "                curr_joint[\r\n",
        "                    1] - offset[1], curr_joint[0] - offset[0],\r\n",
        "                SIGMA, SIGMA)\r\n",
        "\r\n",
        "            heat_maps_other_p[i] = gaussian_heatmap(\r\n",
        "                INPUT_SIZE, INPUT_SIZE,\r\n",
        "                curr_joint[\r\n",
        "                    1] - offset[1], curr_joint[0] - offset[0],\r\n",
        "                SIGMA, SIGMA)\r\n",
        "\r\n",
        "    heat_maps_single_p[-1] = np.maximum(\r\n",
        "        1 - np.max(heat_maps_single_p[:-1], axis=0),\r\n",
        "        np.zeros((INPUT_SIZE, INPUT_SIZE)))\r\n",
        "    heat_maps_single_p = np.transpose(heat_maps_single_p, (1, 2, 0))\r\n",
        "\r\n",
        "    # generate second set of heat-maps for other people in the image\r\n",
        "    for p in range(int(num_other_people)):\r\n",
        "        for i in range(NUM_JOINTS):\r\n",
        "            # the set of joints can be different fromt the one in the json file\r\n",
        "            try:\r\n",
        "                if num_other_people == 1:\r\n",
        "                    curr_joint = joints_other_people[_FILTER_JOINTS[i]]\r\n",
        "                else:\r\n",
        "                    curr_joint = joints_other_people[p][_FILTER_JOINTS[i]]\r\n",
        "                skip = (\r\n",
        "                    curr_joint[0] < 0 or curr_joint[1] < 0 or\r\n",
        "                    curr_joint[0] >= img_width or curr_joint[1] >= img_height)\r\n",
        "            except IndexError:\r\n",
        "                skip = True\r\n",
        "            if not skip:\r\n",
        "                heat_maps_other_p[i] = gaussian_heatmap(\r\n",
        "                    INPUT_SIZE, INPUT_SIZE,\r\n",
        "                    curr_joint[1] - offset[1], curr_joint[0] - offset[0],\r\n",
        "                    SIGMA, SIGMA, init=heat_maps_other_p[i])\r\n",
        "\r\n",
        "    heat_maps_other_p[-1] = np.maximum(\r\n",
        "        1 - np.max(heat_maps_other_p[:-1], axis=0),\r\n",
        "        np.zeros((INPUT_SIZE, INPUT_SIZE)))\r\n",
        "\r\n",
        "    heat_maps_other_p = np.transpose(heat_maps_other_p, (1, 2, 0))\r\n",
        "\r\n",
        "    # rescaling heat-maps accoring to the right shape\r\n",
        "    labels_single = rescale(heat_maps_single_p, OUTPUT_SIZE)\r\n",
        "    labels_people = rescale(heat_maps_other_p, OUTPUT_SIZE)\r\n",
        "    return labels_people, labels_single\r\n",
        "\r\n",
        "\r\n",
        "def generate_center_map(center_pos, img_shape):\r\n",
        "    \"\"\"\r\n",
        "    Given the position of the person and the size of the input image it\r\n",
        "    generates\r\n",
        "    a heat-map where a gaissian distribution is fit in the position of the\r\n",
        "    person in the image.\r\n",
        "    \"\"\"\r\n",
        "    img_height = img_shape\r\n",
        "    img_width = img_shape\r\n",
        "    center_map = gaussian_heatmap(\r\n",
        "        img_height, img_width, center_pos[1], center_pos[0],\r\n",
        "        SIGMA_CENTER, SIGMA_CENTER)\r\n",
        "    return center_map\r\n",
        "\r\n",
        "\r\n",
        "def rescale(data, new_size):\r\n",
        "    \"\"\"Rescale data to a fixed dimension, regardless the number of channels.\r\n",
        "    Data has to be in the format (h,w,c).\"\"\"\r\n",
        "    if data.ndim > 2:\r\n",
        "        assert data.shape[2] < data.shape[0]\r\n",
        "        assert data.shape[2] < data.shape[1]\r\n",
        "    resized_data = cv2.resize(\r\n",
        "        data, (new_size, new_size), interpolation=cv2.INTER_CUBIC)\r\n",
        "    return resized_data\r\n",
        "\r\n",
        "\r\n",
        "def crop_image(image, obj_pose):\r\n",
        "    \"\"\"\r\n",
        "    Crop the image in order to have the person at the center and the final\r\n",
        "    image size\r\n",
        "    is the same as the expected CNN input size.\r\n",
        "    Returns the cropped image and the offset that is used to update the joint\r\n",
        "    positions.\r\n",
        "    \"\"\"\r\n",
        "    offset_left = int(obj_pose[0] - INPUT_SIZE // 2)\r\n",
        "    offset_up = int(obj_pose[1] - INPUT_SIZE // 2)\r\n",
        "    # just for checking that it's inside the image\r\n",
        "    offset_right = int(image.shape[1] - obj_pose[0] - INPUT_SIZE // 2)\r\n",
        "    offset_bottom = int(image.shape[0] - obj_pose[1] - INPUT_SIZE // 2)\r\n",
        "\r\n",
        "    pad_left, pad_right, pad_up, pad_bottom = 0, 0, 0, 0\r\n",
        "    if offset_left < 0:\r\n",
        "        pad_left = -offset_left\r\n",
        "    if offset_right < 0:\r\n",
        "        pad_right = -offset_right\r\n",
        "    if offset_up < 0:\r\n",
        "        pad_up = -offset_up\r\n",
        "    if offset_bottom < 0:\r\n",
        "        pad_bottom = -offset_bottom\r\n",
        "    padded_image = np.lib.pad(\r\n",
        "        image, ((pad_up, pad_bottom), (pad_left, pad_right), (0, 0)),\r\n",
        "        'constant', constant_values=((0, 0), (0, 0), (0, 0)))\r\n",
        "\r\n",
        "    cropped_image = padded_image[\r\n",
        "        offset_up + pad_up: offset_up + pad_up + INPUT_SIZE,\r\n",
        "        offset_left + pad_left: offset_left + pad_left + INPUT_SIZE]\r\n",
        "\r\n",
        "    return cropped_image, np.array([offset_left, offset_up])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk3SHhNAvIut"
      },
      "source": [
        "prob_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urc8i1JwvKsw"
      },
      "source": [
        "import os\r\n",
        "import scipy.io as sio\r\n",
        "import numpy as np\r\n",
        "#from lifting.utils.upright_fast import pick_e\r\n",
        "#from lifting.utils import config\r\n",
        "\r\n",
        "__all__ = ['Prob3dPose']\r\n",
        "\r\n",
        "\r\n",
        "class Prob3dPose:\r\n",
        "\r\n",
        "    def __init__(self, prob_model_path):\r\n",
        "        model_param = sio.loadmat(prob_model_path)\r\n",
        "        self.mu = np.reshape(\r\n",
        "            model_param['mu'], (model_param['mu'].shape[0], 3, -1))\r\n",
        "        self.e = np.reshape(model_param['e'], (model_param['e'].shape[\r\n",
        "                            0], model_param['e'].shape[1], 3, -1))\r\n",
        "        self.sigma = model_param['sigma']\r\n",
        "        self.cam = np.array(\r\n",
        "            [[1.0, 0.0, 0.0], [0.0, 0.0, -1.0], [0.0, 1.0, 0.0]])\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def cost3d(model, gt):\r\n",
        "        \"\"\"3d error in mm\"\"\"\r\n",
        "        out = np.sqrt(((gt - model) ** 2).sum(1)).mean(-1)\r\n",
        "        return out\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def renorm_gt(gt):\r\n",
        "        \"\"\"Compel gt data to have mean joint length of one\"\"\"\r\n",
        "        _POSE_TREE = np.asarray([\r\n",
        "            [0, 1], [1, 2], [2, 3], [0, 4], [4, 5], [5, 6], [0, 7], [7, 8],\r\n",
        "            [8, 9], [9, 10], [8, 11], [11, 12], [12, 13], [8, 14], [14, 15],\r\n",
        "            [15, 16]]).T\r\n",
        "        scale = np.sqrt(((gt[:, :, _POSE_TREE[0]] -\r\n",
        "                          gt[:, :, _POSE_TREE[1]]) ** 2).sum(2).sum(1))\r\n",
        "        return gt / scale[:, np.newaxis, np.newaxis]\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def build_model(a, e, s0):\r\n",
        "        \"\"\"Build 3D model\"\"\"\r\n",
        "        assert (s0.shape[1] == 3)\r\n",
        "        assert (e.shape[2] == 3)\r\n",
        "        assert (a.shape[1] == e.shape[1])\r\n",
        "        out = np.einsum('...i,...ijk', a, e)\r\n",
        "        out += s0\r\n",
        "        return out\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def build_and_rot_model(a, e, s0, r):\r\n",
        "        \"\"\"\r\n",
        "        Build model and rotate according to the identified rotation matrix\r\n",
        "        \"\"\"\r\n",
        "        from numpy.core.umath_tests import matrix_multiply\r\n",
        "\r\n",
        "        r2 = Prob3dPose.upgrade_r(r.T).transpose((0, 2, 1))\r\n",
        "        mod = Prob3dPose.build_model(a, e, s0)\r\n",
        "        mod = matrix_multiply(r2, mod)\r\n",
        "        return mod\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def upgrade_r(r):\r\n",
        "        \"\"\"\r\n",
        "        Upgrades complex parameterisation of planar rotation to tensor\r\n",
        "        containing per frame 3x3 rotation matrices\r\n",
        "        \"\"\"\r\n",
        "        assert (r.ndim == 2)\r\n",
        "        # Technically optional assert, but if this fails data is probably\r\n",
        "        # transposed\r\n",
        "        assert (r.shape[1] == 2)\r\n",
        "        assert (np.all(np.isfinite(r)))\r\n",
        "        norm = np.sqrt((r[:, :2] ** 2).sum(1))\r\n",
        "        assert (np.all(norm > 0))\r\n",
        "        r /= norm[:, np.newaxis]\r\n",
        "        assert (np.all(np.isfinite(r)))\r\n",
        "        newr = np.zeros((r.shape[0], 3, 3))\r\n",
        "        newr[:, :2, 0] = r[:, :2]\r\n",
        "        newr[:, 2, 2] = 1\r\n",
        "        newr[:, 1::-1, 1] = r[:, :2]\r\n",
        "        newr[:, 0, 1] *= -1\r\n",
        "        return newr\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def centre(data_2d):\r\n",
        "        \"\"\"center data according to each of the coordiante components\"\"\"\r\n",
        "        return (data_2d.T - data_2d.mean(1)).T\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def centre_all(data):\r\n",
        "        \"\"\"center all data\"\"\"\r\n",
        "        if data.ndim == 2:\r\n",
        "            return Prob3dPose.centre(data)\r\n",
        "        return (data.transpose(2, 0, 1) - data.mean(2)).transpose(1, 2, 0)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def normalise_data(d2, weights):\r\n",
        "        \"\"\"Normalise data according to height\"\"\"\r\n",
        "\r\n",
        "        # the joints with weight set to 0 should not be considered in the\r\n",
        "        # normalisation process\r\n",
        "        d2 = d2.reshape(d2.shape[0], -1, 2).transpose(0, 2, 1)\r\n",
        "        idx_consider = weights[0, 0].astype(np.bool)\r\n",
        "        if np.sum(weights[:, 0].sum(1) >= MIN_NUM_JOINTS) == 0:\r\n",
        "            raise Exception(\r\n",
        "                'Not enough 2D joints identified to generate 3D pose')\r\n",
        "        d2[:, :, idx_consider] = Prob3dPose.centre_all(d2[:, :, idx_consider])\r\n",
        "\r\n",
        "        # Height normalisation (2 meters)\r\n",
        "        m2 = d2[:, 1, idx_consider].min(1) / 2.0\r\n",
        "        m2 -= d2[:, 1, idx_consider].max(1) / 2.0\r\n",
        "        crap = m2 == 0\r\n",
        "        m2[crap] = 1.0\r\n",
        "        d2[:, :, idx_consider] /= m2[:, np.newaxis, np.newaxis]\r\n",
        "        return d2, m2\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def transform_joints(pose_2d, visible_joints):\r\n",
        "        \"\"\"\r\n",
        "        Transform the set of joints according to what the probabilistic model\r\n",
        "        expects as input.\r\n",
        "\r\n",
        "        It returns the new set of joints of each of the people and the set of\r\n",
        "        weights for the joints.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        _H36M_ORDER = [8, 9, 10, 11, 12, 13, 1, 0, 5, 6, 7, 2, 3, 4]\r\n",
        "        _W_POS = [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, 16]\r\n",
        "\r\n",
        "        def swap_xy(poses):\r\n",
        "            tmp = np.copy(poses[:, :, 0])\r\n",
        "            poses[:, :, 0] = poses[:, :, 1]\r\n",
        "            poses[:, :, 1] = tmp\r\n",
        "            return poses\r\n",
        "\r\n",
        "        assert (pose_2d.ndim == 3)\r\n",
        "        new_pose = pose_2d.copy()\r\n",
        "        new_pose = swap_xy(new_pose)\r\n",
        "        new_pose = new_pose[:, _H36M_ORDER]\r\n",
        "\r\n",
        "        # defining weights according to occlusions\r\n",
        "        weights = np.zeros((pose_2d.shape[0], 2, H36M_NUM_JOINTS))\r\n",
        "        ordered_visibility = np.repeat(\r\n",
        "            visible_joints[:, _H36M_ORDER, np.newaxis], 2, 2\r\n",
        "        ).transpose([0, 2, 1])\r\n",
        "        weights[:, :, _W_POS] = ordered_visibility\r\n",
        "        return new_pose, weights\r\n",
        "\r\n",
        "    def affine_estimate(self, w, depth_reg=0.085, weights=None, scale=10.0,\r\n",
        "                        scale_mean=0.0016 * 1.8 * 1.2, scale_std=1.2 * 0,\r\n",
        "                        cap_scale=-0.00129):\r\n",
        "        \"\"\"\r\n",
        "        Quick switch to allow reconstruction at unknown scale returns a,r\r\n",
        "        and scale\r\n",
        "        \"\"\"\r\n",
        "        weights = np.zeros((0, 0, 0)) if weights is None else weights\r\n",
        "\r\n",
        "        s = np.empty((self.sigma.shape[0], self.sigma.shape[1] + 4))  # e,y,x,z\r\n",
        "        s[:, :4] = 10 ** -5  # Tiny but makes stuff well-posed\r\n",
        "        s[:, 0] = scale_std\r\n",
        "        s[:, 4:] = self.sigma\r\n",
        "        s[:, 4:-1] *= scale\r\n",
        "\r\n",
        "        e2 = np.zeros((self.e.shape[0], self.e.shape[\r\n",
        "                      1] + 4, 3, self.e.shape[3]))\r\n",
        "        e2[:, 1, 0] = 1.0\r\n",
        "        e2[:, 2, 1] = 1.0\r\n",
        "        e2[:, 3, 0] = 1.0\r\n",
        "        # This makes the least_squares problem ill posed, as X,Z are\r\n",
        "        # interchangable\r\n",
        "        # Hence regularisation above to speed convergence and stop blow-up\r\n",
        "        e2[:, 0] = self.mu\r\n",
        "        e2[:, 4:] = self.e\r\n",
        "        t_m = np.zeros_like(self.mu)\r\n",
        "\r\n",
        "        res, a, r = pick_e(w, e2, t_m, self.cam, s, weights=weights,\r\n",
        "                           interval=0.01, depth_reg=depth_reg,\r\n",
        "                           scale_prior=scale_mean)\r\n",
        "\r\n",
        "        scale = a[:, :, 0]\r\n",
        "        reestimate = scale > cap_scale\r\n",
        "        m = self.mu * cap_scale\r\n",
        "        for i in range(scale.shape[0]):\r\n",
        "            if reestimate[i].sum() > 0:\r\n",
        "                ehat = e2[i:i + 1, 1:]\r\n",
        "                mhat = m[i:i + 1]\r\n",
        "                shat = s[i:i + 1, 1:]\r\n",
        "                (res2, a2, r2) = pick_e(\r\n",
        "                    w[reestimate[i]], ehat, mhat, self.cam, shat,\r\n",
        "                    weights=weights[reestimate[i]],\r\n",
        "                    interval=0.01, depth_reg=depth_reg,\r\n",
        "                    scale_prior=scale_mean\r\n",
        "                )\r\n",
        "                res[i:i + 1, reestimate[i]] = res2\r\n",
        "                a[i:i + 1, reestimate[i], 1:] = a2\r\n",
        "                a[i:i + 1, reestimate[i], 0] = cap_scale\r\n",
        "                r[i:i + 1, :, reestimate[i]] = r2\r\n",
        "        scale = a[:, :, 0]\r\n",
        "        a = a[:, :, 1:] / a[:, :, 0][:, :, np.newaxis]\r\n",
        "        return res, e2[:, 1:], a, r, scale\r\n",
        "\r\n",
        "    def better_rec(self, w, model, s=1, weights=1, damp_z=1):\r\n",
        "        \"\"\"Quick switch to allow reconstruction at unknown scale\r\n",
        "        returns a,r and scale\"\"\"\r\n",
        "        from numpy.core.umath_tests import matrix_multiply\r\n",
        "        proj = matrix_multiply(self.cam[np.newaxis], model)\r\n",
        "        proj[:, :2] = (proj[:, :2] * s + w * weights) / (s + weights)\r\n",
        "        proj[:, 2] *= damp_z\r\n",
        "        out = matrix_multiply(self.cam.T[np.newaxis], proj)\r\n",
        "        return out\r\n",
        "\r\n",
        "    def create_rec(self, w2, weights, res_weight=1):\r\n",
        "        \"\"\"Reconstruct 3D pose given a 2D pose\"\"\"\r\n",
        "        _SIGMA_SCALING = 5.2\r\n",
        "\r\n",
        "        res, e, a, r, scale = self.affine_estimate(\r\n",
        "            w2, scale=_SIGMA_SCALING, weights=weights,\r\n",
        "            depth_reg=0, cap_scale=-0.001, scale_mean=-0.003\r\n",
        "        )\r\n",
        "\r\n",
        "        remaining_dims = 3 * w2.shape[2] - e.shape[1]\r\n",
        "        assert (remaining_dims >= 0)\r\n",
        "        llambda = -np.log(self.sigma)\r\n",
        "        lgdet = np.sum(llambda[:, :-1], 1) + llambda[:, -1] * remaining_dims\r\n",
        "        score = (res * res_weight + lgdet[:, np.newaxis] * (scale ** 2))\r\n",
        "        best = np.argmin(score, 0)\r\n",
        "        index = np.arange(best.shape[0])\r\n",
        "        a2 = a[best, index]\r\n",
        "        r2 = r[best, :, index].T\r\n",
        "        rec = Prob3dPose.build_and_rot_model(a2, e[best], self.mu[best], r2)\r\n",
        "        rec *= -np.abs(scale[best, index])[:, np.newaxis, np.newaxis]\r\n",
        "\r\n",
        "        rec = self.better_rec(w2, rec, 1, 1.55 * weights, 1) * -1\r\n",
        "        rec = Prob3dPose.renorm_gt(rec)\r\n",
        "        rec *= 0.97\r\n",
        "        return rec\r\n",
        "\r\n",
        "    def compute_3d(self, pose_2d, weights):\r\n",
        "        \"\"\"Reconstruct 3D poses given 2D estimations\"\"\"\r\n",
        "\r\n",
        "        _J_POS = [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15, 16]\r\n",
        "        _SCALE_3D = 1174.88312988\r\n",
        "\r\n",
        "        if pose_2d.shape[1] != H36M_NUM_JOINTS:\r\n",
        "            # need to call the linear regressor\r\n",
        "            reg_joints = np.zeros(\r\n",
        "                (pose_2d.shape[0], H36M_NUM_JOINTS, 2))\r\n",
        "            for oid, singe_pose in enumerate(pose_2d):\r\n",
        "                reg_joints[oid, _J_POS] = singe_pose\r\n",
        "\r\n",
        "            norm_pose, _ = Prob3dPose.normalise_data(reg_joints, weights)\r\n",
        "        else:\r\n",
        "            norm_pose, _ = Prob3dPose.normalise_data(pose_2d, weights)\r\n",
        "\r\n",
        "        pose_3d = self.create_rec(norm_pose, weights) * _SCALE_3D\r\n",
        "        return pose_3d\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WCB5AWpxAOZ"
      },
      "source": [
        "_pose_estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9YkI3Rwx_Uc"
      },
      "source": [
        "#from . import utils\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import abc\r\n",
        "ABC = abc.ABCMeta('ABC', (object,), {})\r\n",
        "\r\n",
        "__all__ = [\r\n",
        "    'PoseEstimatorInterface',\r\n",
        "    'PoseEstimator'\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "class PoseEstimatorInterface(ABC):\r\n",
        "\r\n",
        "    @abc.abstractmethod\r\n",
        "    def initialise(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abc.abstractmethod\r\n",
        "    def estimate(self, image):\r\n",
        "        return\r\n",
        "\r\n",
        "    @abc.abstractmethod\r\n",
        "    def close(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "\r\n",
        "class PoseEstimator(PoseEstimatorInterface):\r\n",
        "\r\n",
        "    def __init__(self, image_size, session_path, prob_model_path):\r\n",
        "        \"\"\"Initialising the graph in tensorflow.\r\n",
        "        INPUT:\r\n",
        "            image_size: Size of the image in the format (w x h x 3)\"\"\"\r\n",
        "\r\n",
        "        self.session = None\r\n",
        "        self.poseLifting = Prob3dPose(prob_model_path)\r\n",
        "        self.sess = -1\r\n",
        "        self.orig_img_size = np.array(image_size)\r\n",
        "        self.scale = INPUT_SIZE / (self.orig_img_size[0] * 1.0)\r\n",
        "        self.img_size = np.round(\r\n",
        "            self.orig_img_size * self.scale).astype(np.int32)\r\n",
        "        self.image_in = None\r\n",
        "        self.heatmap_person_large = None\r\n",
        "        self.pose_image_in = None\r\n",
        "        self.pose_centermap_in = None\r\n",
        "        self.pred_2d_pose = None\r\n",
        "        self.likelihoods = None\r\n",
        "        self.session_path = session_path\r\n",
        "\r\n",
        "    def initialise(self):\r\n",
        "        \"\"\"Load saved model in the graph\r\n",
        "        INPUT:\r\n",
        "            sess_path: path to the dir containing the tensorflow saved session\r\n",
        "        OUTPUT:\r\n",
        "            sess: tensorflow session\"\"\"\r\n",
        "\r\n",
        "        tf.reset_default_graph()\r\n",
        "        with tf.variable_scope('CPM'):\r\n",
        "            # placeholders for person network\r\n",
        "            self.image_in = tf.placeholder(\r\n",
        "                tf.float32, [1, INPUT_SIZE, self.img_size[1], 3])\r\n",
        "\r\n",
        "            heatmap_person = inference_person(self.image_in)\r\n",
        "\r\n",
        "            self.heatmap_person_large = tf.image.resize_images(\r\n",
        "                heatmap_person, [INPUT_SIZE, self.img_size[1]])\r\n",
        "\r\n",
        "            # placeholders for pose network\r\n",
        "            self.pose_image_in = tf.placeholder(\r\n",
        "                tf.float32,\r\n",
        "                [BATCH_SIZE, INPUT_SIZE, INPUT_SIZE, 3])\r\n",
        "\r\n",
        "            self.pose_centermap_in = tf.placeholder(\r\n",
        "                tf.float32,\r\n",
        "                [BATCH_SIZE, INPUT_SIZE, INPUT_SIZE, 1])\r\n",
        "\r\n",
        "            self.pred_2d_pose, self.likelihoods = inference_pose(\r\n",
        "                self.pose_image_in, self.pose_centermap_in,\r\n",
        "                INPUT_SIZE)\r\n",
        "\r\n",
        "        sess = tf.Session()\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        saver = tf.train.Saver()\r\n",
        "        saver.restore(sess, self.session_path)\r\n",
        "\r\n",
        "        self.session = sess\r\n",
        "\r\n",
        "    def estimate(self, image):\r\n",
        "        \"\"\"\r\n",
        "        Estimate 2d and 3d poses on the image.\r\n",
        "        INPUT:\r\n",
        "            image: RGB image in the format (w x h x 3)\r\n",
        "            sess: tensorflow session\r\n",
        "        OUTPUT:\r\n",
        "            pose_2d: 2D pose for each of the people in the image in the format\r\n",
        "            (num_ppl x num_joints x 2) visibility: vector containing a bool\r\n",
        "            value for each joint representing the visibility of the joint in\r\n",
        "            the image (could be due to occlusions or the joint is not in the\r\n",
        "            image) pose_3d: 3D pose for each of the people in the image in the\r\n",
        "            format (num_ppl x 3 x num_joints)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        sess = self.session\r\n",
        "\r\n",
        "        image = cv2.resize(image, (0, 0), fx=self.scale,\r\n",
        "                           fy=self.scale, interpolation=cv2.INTER_CUBIC)\r\n",
        "        b_image = np.array(image[np.newaxis] / 255.0 - 0.5, dtype=np.float32)\r\n",
        "\r\n",
        "        hmap_person = sess.run(self.heatmap_person_large, {\r\n",
        "                               self.image_in: b_image})\r\n",
        "\r\n",
        "        hmap_person = np.squeeze(hmap_person)\r\n",
        "        centers = detect_objects_heatmap(hmap_person)\r\n",
        "        b_pose_image, b_pose_cmap = prepare_input_posenet(\r\n",
        "            b_image[0], centers,\r\n",
        "            [INPUT_SIZE, image.shape[1]],\r\n",
        "            [INPUT_SIZE, INPUT_SIZE],\r\n",
        "            batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        "        feed_dict = {\r\n",
        "            self.pose_image_in: b_pose_image,\r\n",
        "            self.pose_centermap_in: b_pose_cmap\r\n",
        "        }\r\n",
        "\r\n",
        "        # Estimate 2D poses\r\n",
        "        pred_2d_pose, pred_likelihood = sess.run([self.pred_2d_pose,\r\n",
        "                                                  self.likelihoods],\r\n",
        "                                                 feed_dict)\r\n",
        "        estimated_2d_pose, visibility = detect_parts_from_likelihoods(pred_2d_pose,\r\n",
        "                                                                            centers,\r\n",
        "                                                                            pred_likelihood)\r\n",
        "\r\n",
        "        # Estimate 3D poses\r\n",
        "        transformed_pose2d, weights = self.poseLifting.transform_joints(\r\n",
        "            estimated_2d_pose.copy(), visibility)\r\n",
        "        pose_3d = self.poseLifting.compute_3d(transformed_pose2d, weights)\r\n",
        "        pose_2d = np.round(estimated_2d_pose / self.scale).astype(np.int32)\r\n",
        "\r\n",
        "        return pose_2d, visibility, pose_3d\r\n",
        "\r\n",
        "    def close(self):\r\n",
        "        self.session.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANZpKIgQyIjY"
      },
      "source": [
        "#DEMO\r\n",
        "\r\n",
        "running Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bwx9CC5Ds4D"
      },
      "source": [
        "!wget -o \"https://drive.google.com/file/d/1NUHSl0HhSF2L3ywwUNuBscHzintNmGjQ/view?usp=sharing\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DisiLvAoYF8n"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejaQ6yP8DbEn"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwF49OvJyTwI"
      },
      "source": [
        "import __init__\r\n",
        "\r\n",
        "'''from lifting import PoseEstimator\r\n",
        "from lifting.utils import draw_limbs\r\n",
        "from lifting.utils import plot_pose\r\n",
        "'''\r\n",
        "import cv2\r\n",
        "import numpy\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from os.path import dirname, realpath\r\n",
        "\r\n",
        "'''DIR_PATH = dirname(realpath(__file__))\r\n",
        "PROJECT_PATH = realpath(DIR_PATH + '/..')\r\n",
        "SAVED_SESSIONS_DIR = 'pose3d/data/saved_sessions'\r\n",
        "SESSION_PATH = SAVED_SESSIONS_DIR + '/init_session/init'\r\n",
        "PROB_MODEL_PATH = SAVED_SESSIONS_DIR + '/prob_model/prob_model_params.mat'\r\n",
        "'''\r\n",
        "\r\n",
        "IMAGE_FILE_PATH = 'test_image.png' #PROJECT_PATH + '/data/images/test_image.png'\r\n",
        "SAVED_SESSIONS_DIR = '/content/gdrive/MyDrive/Pose3d/data/saved_sessions'\r\n",
        "SESSION_PATH = '/content/init' #'/content/gdrive/MyDrive/Pose3d/data/saved_sessions/init_session/init'\r\n",
        "PROB_MODEL_PATH = '/content/prob_model_params.mat' #'/content/gdrive/MyDrive/Pose3d/data/saved_sessions/prob_model/prob_model_params.mat'\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    image = cv2.imread(IMAGE_FILE_PATH)\r\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # conversion to rgb\r\n",
        "\r\n",
        "    # create pose estimator\r\n",
        "    image_size = image.shape\r\n",
        "\r\n",
        "    pose_estimator = PoseEstimator(image_size, SESSION_PATH, PROB_MODEL_PATH)\r\n",
        "\r\n",
        "    # load model\r\n",
        "    pose_estimator.initialise()\r\n",
        "\r\n",
        "    try:\r\n",
        "        # estimation\r\n",
        "        pose_2d, visibility, pose_3d = pose_estimator.estimate(image)\r\n",
        "\r\n",
        "        # Show 2D and 3D poses\r\n",
        "        display_results(image, pose_2d, visibility, pose_3d)\r\n",
        "    except ValueError:\r\n",
        "        print('No visible people in the image. Change CENTER_TR in packages/lifting/utils/config.py ...')\r\n",
        "\r\n",
        "    # close model\r\n",
        "    pose_estimator.close()\r\n",
        "\r\n",
        "\r\n",
        "def display_results(in_image, data_2d, joint_visibility, data_3d):\r\n",
        "    \"\"\"Plot 2D and 3D poses for each of the people in the image.\"\"\"\r\n",
        "    plt.figure()\r\n",
        "    draw_limbs(in_image, data_2d, joint_visibility)\r\n",
        "    plt.imshow(in_image)\r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "    # Show 3D poses\r\n",
        "    for single_3D in data_3d:\r\n",
        "        # or plot_pose(Prob3dPose.centre_all(single_3D))\r\n",
        "        plot_pose(single_3D)\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    import sys\r\n",
        "    sys.exit(main())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}